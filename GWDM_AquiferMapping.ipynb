{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Hydroinformatics/gwdm-notebooks/blob/main/GWDM_AquiferMapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLC55BdBCaKl"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyApLXOWCSDZ"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Installing packages**\n",
        "%%capture\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install rioxarray\n",
        "!pip install gstools\n",
        "!pip install matplotlib\n",
        "#!pip install kats\n",
        "!pip install pyod\n",
        "!pip install netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04I7TAGFCkIP"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Importing packages**\n",
        "import calendar\n",
        "import copy\n",
        "import datetime\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "import urllib\n",
        "from pathlib import Path\n",
        "from timeit import default_timer as timer\n",
        "from urllib import request\n",
        "from xml.etree import cElementTree as ET\n",
        "\n",
        "import geopandas as gpd\n",
        "import gstools as gs\n",
        "import netCDF4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xarray\n",
        "import rioxarray\n",
        "from scipy import interpolate\n",
        "from shapely import wkt\n",
        "from shapely.geometry import mapping\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhGS6o2bEhG6"
      },
      "source": [
        "# Uploading Aquifer Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH0Vj3NcCxFp"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Uploading Aquifer File**\n",
        "upload_aquifers = files.upload()\n",
        "aquifers = gpd.GeoDataFrame.from_file(list(upload_aquifers.keys())[0])\n",
        "\n",
        "aList1 = list(aquifers.columns)\n",
        "#aList1.append('NA')\n",
        "a_ID = widgets.Dropdown(options=aList1, description = \"Aquifer ID\")\n",
        "a_name = widgets.Dropdown(options=aList1, description = \"Aquifer Name\")\n",
        "\n",
        "a1Items = [a_ID, a_name]\n",
        "print(\"\\nPlease select the appropriate headers for your file\")\n",
        "widgets.GridBox(a1Items, layout=widgets.Layout(grid_template_columns=\"repeat(1, 550px)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tADsIEeHkbr-"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Select the Aquifer you would like to Map**\n",
        "aList2 = list(aquifers[a_name.value])\n",
        "aquifer_name = widgets.Dropdown(options=aList2, description = \"Aquifer Name\")\n",
        "\n",
        "a2Items = [aquifer_name]\n",
        "print(\"\\nPlease select the aquifer you would like to map\")\n",
        "widgets.GridBox(a2Items, layout=widgets.Layout(grid_template_columns=\"repeat(1, 550px)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6cTAFF1j8tV"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Filter and Display Aquifer Information**\n",
        "aquifer_index = aquifers[aquifers[a_name.value]==aquifer_name.value].index.values\n",
        "aquifer = aquifers[aquifers[a_name.value].isin([aquifer_name.value])]\n",
        "aquifer_ID = list(aquifer[a_ID.value])\n",
        "aquifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtMFJ3UFDY8T"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Uploading wells file**\n",
        "upload_wells = files.upload()\n",
        "wells = pd.read_csv(list(upload_wells.keys())[0])\n",
        "\n",
        "wList = list(wells.columns)\n",
        "wList.append('NA')\n",
        "w_well_ID = widgets.Dropdown(options=wList, description = \"Well ID\")\n",
        "w_well_name = widgets.Dropdown(options=wList, description = \"Well Name\")\n",
        "w_lat = widgets.Dropdown(options=wList, description = \"Latitude\")\n",
        "w_long = widgets.Dropdown(options=wList, description = \"Longitude\")\n",
        "w_aquifer_ID = widgets.Dropdown(options=wList, description = \"Aquifer ID\")\n",
        "\n",
        "wItems = [w_well_ID, w_well_name, w_lat, w_long, w_aquifer_ID]\n",
        "print(\"\\nPlease select the appropriate headers for your file\")\n",
        "widgets.GridBox(wItems, layout=widgets.Layout(grid_template_columns=\"repeat(1, 550px)\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To avoid type mismatch, convert the aquifer_id found from the aquifer file and the aquiferid column\n",
        "#in the wells dataframe to both be strings\n",
        "aquifer_ID = [str(item) for item in aquifer_ID]\n",
        "if (w_aquifer_ID.value !=\"NA\"):\n",
        "  wells[w_aquifer_ID.value] = wells[w_aquifer_ID.value].astype(str)"
      ],
      "metadata": {
        "id": "aU45jHLCHyJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWepWYSG7ynf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Formatting wells dataframe**\n",
        "wells_gdf = gpd.GeoDataFrame(wells, geometry=gpd.points_from_xy(wells[w_long.value], wells[w_lat.value]))\n",
        "if (w_aquifer_ID.value !=\"NA\"):\n",
        "  wells_gdf = wells_gdf[wells_gdf[w_aquifer_ID.value].isin(aquifer_ID)]\n",
        "wells_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGDfk069Dxo7"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Plotting aquifer and wells**\n",
        "fig, ax = plt.subplots()\n",
        "aquifer.plot(color=\"none\", edgecolor=\"red\", ax=ax)\n",
        "wells_gdf.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFTl7W_ID-QV"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Uploading timeseries**\n",
        "upload_timeseries = files.upload()\n",
        "measurements = pd.read_csv(list(upload_timeseries.keys())[0])\n",
        "\n",
        "tList = list(measurements.columns)\n",
        "tList.append('NA')\n",
        "ts_well_ID = widgets.Dropdown(options=tList, description = \"Well ID\")\n",
        "ts_date = widgets.Dropdown(options=tList, description = \"Date\")\n",
        "ts_measurement = widgets.Dropdown(options=tList, description = \"Measurement\")\n",
        "ts_aquifer_ID = widgets.Dropdown(options=tList, description = \"Aquifer ID\")\n",
        "\n",
        "tItems = [ts_well_ID, ts_date, ts_measurement, ts_aquifer_ID]\n",
        "print(\"\\nPlease select the appropriate headers for your file\")\n",
        "widgets.GridBox(tItems, layout=widgets.Layout(grid_template_columns=\"repeat(1, 550px)\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To avoid type mismatch datatypes\n",
        "if (ts_aquifer_ID.value !=\"NA\"):\n",
        "  measurements[ts_aquifer_ID.value] = measurements[ts_aquifer_ID.value].astype(str)"
      ],
      "metadata": {
        "id": "g_GjmAxrH6PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvCf3wWq9EfI"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Formatting Timeseries**\n",
        "measurements[ts_date.value] = pd.to_datetime(measurements[ts_date.value], infer_datetime_format=True)\n",
        "if (ts_aquifer_ID.value !=\"NA\"):\n",
        "  measurements = measurements[measurements[ts_aquifer_ID.value].isin(aquifer_ID)]\n",
        "measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUoMfX_7rUiT"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Dropping measurements that don't have corresponding wells**\n",
        "no_uniquewells = len(measurements[ts_well_ID.value].unique())\n",
        "\n",
        "measurements = measurements[measurements[ts_well_ID.value].isin(wells_gdf[w_well_ID.value])].reset_index(drop=True)\n",
        "\n",
        "no_uniquewellsref = len(measurements[ts_well_ID.value].unique())\n",
        "\n",
        "print(no_uniquewells, ' unique well IDs')\n",
        "print(no_uniquewellsref, ' wells with measurements')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oSloAWjEOb1"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Defining aquifer bounds**\n",
        "bbox = aquifer.bounds.values[0].tolist()\n",
        "bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvdh4c0JGPEc"
      },
      "source": [
        "# Accessing Data from Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JPbXu32uGfMs"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Defining functions for this section**\n",
        "\n",
        "def get_pdsi_df(aquifer_gdf):\n",
        "    data_dir = \"/content\"\n",
        "    nc_file = os.path.join(data_dir, \"pdsi.nc4\")\n",
        "    pdsi_ds = xarray.open_dataset(nc_file, decode_times=False)\n",
        "    units, _, reference_date = pdsi_ds.time.attrs[\"units\"].split(\"since\")\n",
        "    pdsi_ds[\"time\"] = pd.date_range(\n",
        "        start=reference_date, periods=pdsi_ds.sizes[\"time\"], freq=\"MS\"\n",
        "    )\n",
        "    ds = pdsi_ds[\"pdsi_filled\"].to_dataset()\n",
        "    ds[\"pdsi_filled\"] = ds[\"pdsi_filled\"].rio.write_crs(\"epsg:4326\")\n",
        "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
        "    # aquifer_geom = wkt.loads(aquifer_obj[0])\n",
        "    # aquifer_gdf = gpd.GeoDataFrame(\n",
        "    #     {\"name\": [\"random\"], \"geometry\": [aquifer_geom]}, crs=\"EPSG:4326\"\n",
        "    # )\n",
        "    clipped_ds = ds.rio.clip(\n",
        "        aquifer_gdf.geometry.apply(mapping),\n",
        "        aquifer_gdf.crs,\n",
        "        drop=True,\n",
        "        all_touched=True,\n",
        "    ).drop(\"spatial_ref\")\n",
        "    filled_df = (\n",
        "        clipped_ds.to_dataframe()\n",
        "        .reset_index()\n",
        "        .groupby(\"time\")[\"pdsi_filled\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "        .set_index(\"time\")\n",
        "    )\n",
        "    return filled_df\n",
        "\n",
        "# outdated function sarva wanted replaced\n",
        "def get_time_bounds(url):\n",
        "    # This function returns the first and last available time\n",
        "    # from a url of a getcapabilities page located on a Thredds Server\n",
        "    f = urllib.request.urlopen(url)\n",
        "    tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "    # These lines of code find the time dimension information for the netcdf on the Thredds server\n",
        "    dim = root.findall(\".//{http://www.opengis.net/wms}Dimension\")\n",
        "    dim = dim[0].text\n",
        "    times = dim.split(\",\")\n",
        "    times.pop(0)\n",
        "    timemin = times[0]\n",
        "    timemax = times[-1]\n",
        "    # timemin and timemax are the first and last available times on the specified url\n",
        "    return timemin, timemax\n",
        "#\n",
        "# # sarva's new function\n",
        "# def get_time_bounds(url):\n",
        "#     # This function returns the first and last available time\n",
        "#     # from a url of a getcapabilities page located on a Thredds Server\n",
        "#     f = urllib.request.urlopen(url)\n",
        "#     tree = ET.parse(f)\n",
        "#     root = tree.getroot()\n",
        "#     # These lines of code find the time dimension information for the netcdf on the Thredds server\n",
        "#     dim = root.findall(\".//{http://www.opengis.net/wms}Dimension\")\n",
        "#     dim = dim[0].text\n",
        "#     times = dim.split(\",\")\n",
        "#     timemin = re.sub(r\"[\\n\\t\\s]*\", \"\", times[0])\n",
        "#     timemax = re.sub(r\"[\\n\\t\\s]*\", \"\", times[-1])\n",
        "#     # timemin and timemax are the first and last available times on the specified url\n",
        "#     return timemin, timemax\n",
        "\n",
        "\n",
        "\n",
        "def get_thredds_value(server, layer, bbox):\n",
        "    # This function returns a pandas dataframe of the timeseries values of a specific layer\n",
        "    # at a specific latitude and longitude from a file on a Thredds server\n",
        "    # server: the url of the netcdf desired netcdf file on the Thredds server to read\n",
        "    # layer: the name of the layer to extract timeseries information from for the netcdf file\n",
        "    # lat: the latitude of the point at which to extract the timeseries\n",
        "    # lon: the longitude of the point at which to extract the timeseries\n",
        "    # returns df: a pandas dataframe of the timeseries at lat and lon for the layer in the server netcdf file\n",
        "    # calls the getTimeBounds function to get the first and last available times for the netcdf file on the server\n",
        "    time_min, time_max = get_time_bounds(\n",
        "        server + \"?service=WMS&version=1.3.0&request=GetCapabilities\"\n",
        "    )\n",
        "    print(time_min)\n",
        "    print(time_max)\n",
        "    # These lines properly format a url request for the timeseries of a speific layer from a netcdf on\n",
        "    # a Thredds server\n",
        "    server = f\"{server}?service=WMS&version=1.3.0&request=GetFeatureInfo&CRS=CRS:84&QUERY_LAYERS={layer}\"\n",
        "    server = f\"{server}&X=0&Y=0&I=0&J=0&BBOX={bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}&LAYER={layer}\"\n",
        "    server = f\"{server}&WIDTH=1&Height=1&INFO_FORMAT=text/xml&STYLES=raster/default\"\n",
        "    server = f\"{server}&TIME={time_min}/{time_max}\"\n",
        "    print(server)\n",
        "    f = request.urlopen(server)\n",
        "    tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "    features = root.findall(\"FeatureInfo\")\n",
        "    times = []\n",
        "    values = []\n",
        "    for child in features:\n",
        "        time = datetime.datetime.strptime(child[0].text, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        times.append(time)\n",
        "        values.append(child[1].text)\n",
        "\n",
        "    df = pd.DataFrame(index=times, columns=[layer], data=values)\n",
        "    df[layer] = df[layer].replace(\"none\", np.nan).astype(float)\n",
        "    return df\n",
        "\n",
        "def sat_resample(gldas_df):\n",
        "    # resamples the data from both datasets to a monthly value,\n",
        "    # uses the mean of all measurements in a month\n",
        "    # first resample to daily values, then take the start of the month\n",
        "    gldas_df = gldas_df.resample(\"D\").mean()\n",
        "    # D is daily, mean averages any values in a given day, if no data in that day, gives NaN\n",
        "\n",
        "    gldas_df.interpolate(method=\"pchip\", inplace=True, limit_area=\"inside\")\n",
        "\n",
        "    gldas_df = gldas_df.resample(\"MS\").first()\n",
        "    # MS means \"month start\" or to the start of the month, this is the interpolated value\n",
        "\n",
        "    return gldas_df\n",
        "\n",
        "def sat_rolling_window(gldas_df):\n",
        "    years = [1, 3, 5, 10]\n",
        "    names = list(gldas_df.columns)\n",
        "    new_names = copy.deepcopy(\n",
        "        names\n",
        "    )  # names is a list of the varibiles in the data frame, need to unlink for loop\n",
        "    # This loop adds the yearly, 3-year, 5-year, and 10-year rolling averages of each variable to the dataframe\n",
        "    # rolling averages uses the preceding time to compute the last value,\n",
        "    # e.g., using the preceding 5 years of data to get todays\n",
        "    for name in names:\n",
        "        for year in years:\n",
        "            new = name + \"_yr\" + str(year).zfill(2)\n",
        "            gldas_df[new] = gldas_df[name].rolling(year * 12).mean()\n",
        "            new_names.append(new)\n",
        "    return gldas_df, new_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0wgNQGlGVXL"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Downloading PDSI data from server**\n",
        "pdsi_response = requests.get(\"https://github.com/BYU-Hydroinformatics/gwdm-data/blob/main/pdsi.nc4?raw=true\")\n",
        "with open(\"pdsi.nc4\", \"wb\") as binary_file:\n",
        "    # Write bytes to file\n",
        "    binary_file.write(pdsi_response.content)\n",
        "pdsi_df = get_pdsi_df(aquifer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf5EpzZLG791"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Downloading soil moisture data from server**\n",
        "SERVER2 = \"https://tethyswa.servirglobal.net/thredds/wms/testAll/psl/soilw.mon.mean.nc\"\n",
        "#SERVER2 = \"https://www.psl.noaa.gov/thredds/wms/Datasets/cpcsoil/soilw.mon.mean.nc\"\n",
        "LAYER2 = \"soilw\"  # name of data column to be returned\n",
        "soilw_df = get_thredds_value(SERVER2, LAYER2, bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ncaaUXSHyW_"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Resampling the data grid size and adding rolling average**\n",
        "gldas_df = pd.concat([pdsi_df, soilw_df], join=\"outer\", axis=1)\n",
        "gldas_df = sat_resample(gldas_df)\n",
        "gldas_df, names = sat_rolling_window(gldas_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQDvbJCQIaoB"
      },
      "source": [
        "# Temporal Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ5d6JG8I00d"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Setting parameters for temporal interpolation**\n",
        "min_samples = 5 #@param {type:\"integer\"}\n",
        "gap_size = \"3650\" #@param {type:\"string\"}\n",
        "pad = 365 #@param {type:\"integer\"}\n",
        "spacing = \"1MS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fQE3F7EuI77v"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Defining functions for this section**\n",
        "\n",
        "# def smooth(y, box_size):\n",
        "#     \"\"\"moving window function using convolution\n",
        "#     y:          1D vector of values\n",
        "#     box_size:   size of the moving window, should be an odd integer, will set if not\n",
        "#     this function pads the data so that the edges are equal to the\n",
        "#     orginal value\n",
        "#     \"\"\"\n",
        "#     if int(box_size) % 2 == 0:  # make sure box_size is odd integer\n",
        "#         box_size = int(box_size) + 1  # if even, add one\n",
        "#     else:\n",
        "#         box_size = int(box_size)  # if not even (e.g., float), make integer\n",
        "\n",
        "#     box = np.ones(box_size) / box_size  # convolution kernal for moving window\n",
        "#     y_pad = np.pad(\n",
        "#         y, (box_size // 2, box_size - 1 - box_size // 2), mode=\"edge\"\n",
        "#     )  # padd for edge effects\n",
        "#     y_smooth = np.convolve(\n",
        "#         y_pad, box, mode=\"valid\"\n",
        "#     )  # convolve, 'valid' key work trims pad\n",
        "#     return y_smooth\n",
        "\n",
        "# def plot_anomalies(df, x='date', y='amount'):\n",
        "\n",
        "#     # categories will be having values from 0 to n\n",
        "#     # for each values in 0 to n it is mapped in colormap\n",
        "#     categories = df['Predictions'].to_numpy()\n",
        "#     colormap = np.array(['g', 'r'])\n",
        "\n",
        "#     f = plt.figure(figsize=(12, 4))\n",
        "#     f = plt.scatter(df[x], df[y], c=colormap[categories])\n",
        "#     f = plt.xlabel(x)\n",
        "#     f = plt.ylabel(y)\n",
        "#     # f = plt.xticks(rotation=90)\n",
        "#     plt.show()\n",
        "\n",
        "# def extract_query_objects(region_id, aquifer_id, variable):\n",
        "#     session = get_session_obj()\n",
        "#     aquifer_obj = (\n",
        "#         session.query(gf2.ST_AsText(Aquifer.geometry), Aquifer.aquifer_name)\n",
        "#         .filter(Aquifer.region_id == region_id, Aquifer.id == aquifer_id)\n",
        "#         .first()\n",
        "#     )\n",
        "#     bbox = wkt.loads(aquifer_obj[0]).bounds\n",
        "#     wells_query = session.query(Well).filter(Well.aquifer_id == aquifer_id)\n",
        "#     wells_query_df = pd.read_sql(wells_query.statement, session.bind)\n",
        "#     well_ids = [int(well_id) for well_id in wells_query_df.id.values]\n",
        "#     # well_dict = {well.id: well.gse for well in wells_query_df.itertuples()}\n",
        "#     m_query = session.query(Measurement).filter(\n",
        "#         Measurement.well_id.in_(well_ids), Measurement.variable_id == variable\n",
        "#     )\n",
        "#     measurements_df = pd.read_sql(m_query.statement, session.bind)\n",
        "#     # measurements_df['gse'] = measurements_df['well_id'].map(well_dict)\n",
        "#     measurements_df[\"date\"] = pd.to_datetime(\n",
        "#         measurements_df.ts_time, infer_datetime_format=True\n",
        "#     )\n",
        "\n",
        "#     session.close()\n",
        "\n",
        "#     return bbox, wells_query_df, measurements_df, aquifer_obj    \n",
        "\n",
        "def extract_well_data(name, well_df, min_samples=0, ts_col=\"ts_value\", date_col=\"date\"):\n",
        "    if len(well_df) >= min_samples:\n",
        "        # if (well_df['date'].min() < start_date) and (well_df['date'].max() > end_date):\n",
        "        # elevation = well_df['gse'].unique()[0]\n",
        "        df = pd.DataFrame(\n",
        "            index=well_df[date_col].values,\n",
        "            data=well_df[ts_col].values,\n",
        "            columns=[name],\n",
        "        )\n",
        "        df = df[np.logical_not(df.index.duplicated())]\n",
        "        return df\n",
        "\n",
        "def interp_well(wells_df, gap_size, pad, spacing):\n",
        "    well_interp_df = pd.DataFrame()\n",
        "    # create a time index to interpolate over - cover entire range\n",
        "    interp_index: pd.DatetimeIndex = pd.date_range(\n",
        "        start=min(wells_df.index), freq=spacing, end=max(wells_df.index)\n",
        "    )\n",
        "    # loop over each well, interpolate data using pchip\n",
        "    for well in wells_df:\n",
        "        temp_df = wells_df[well].dropna()  # available data for a well\n",
        "\n",
        "        x_index = temp_df.index.astype(\"int\")  # dates for available data\n",
        "\n",
        "        x_diff = temp_df.index.to_series().diff()  # data gap sizes\n",
        "\n",
        "        fit2 = interpolate.pchip(x_index, temp_df)  # pchip fit to data\n",
        "\n",
        "        ynew = fit2(interp_index.astype(\"int\"))  # interpolated data on full range\n",
        "\n",
        "        interp_df = pd.DataFrame(ynew, index=interp_index, columns=[well])\n",
        "\n",
        "        # replace data in gaps of > 1 year with nans\n",
        "\n",
        "        gaps = np.where(x_diff > gap_size)  # list of indexes where gaps are large\n",
        "\n",
        "        for g in gaps[0]:\n",
        "            start = x_diff.index[g - 1] + datetime.timedelta(days=pad)\n",
        "\n",
        "            end = x_diff.index[g] - datetime.timedelta(days=pad)\n",
        "\n",
        "            interp_df[start:end] = np.nan\n",
        "\n",
        "        beg_meas_date = x_diff.index[0]  # date of 1st measured point\n",
        "\n",
        "        end_meas_date = temp_df.index[-1]  # date of last measured point\n",
        "\n",
        "        mask1 = (\n",
        "            interp_df.index < beg_meas_date\n",
        "        )  # locations of data before 1st measured point\n",
        "\n",
        "        interp_df[mask1] = np.nan  # blank out data before 1st measured point\n",
        "\n",
        "        mask2 = (\n",
        "            interp_df.index >= end_meas_date\n",
        "        )  # locations of data after the last measured point\n",
        "\n",
        "        interp_df[mask2] = np.nan  # blank data from last measured point\n",
        "\n",
        "        # add the interp_df data to the full data frame\n",
        "\n",
        "        well_interp_df = pd.concat(\n",
        "            [well_interp_df, interp_df], join=\"outer\", axis=1, sort=False\n",
        "        )\n",
        "    return well_interp_df\n",
        "\n",
        "def norm_training_data(in_df, ref_df):\n",
        "    norm_in_df = (in_df - ref_df.min().values) / (\n",
        "        ref_df.max().values - ref_df.min().values\n",
        "    )  # use values as df sometimes goofs\n",
        "    return norm_in_df\n",
        "\n",
        "def input_to_hidden(x, Win, b):\n",
        "    # setup matrixes\n",
        "    a = np.dot(x, Win) + b\n",
        "    a = np.maximum(a, 0, a)  # relu\n",
        "    return a\n",
        "\n",
        "def predict(in_values, W_in, b, W_out):\n",
        "    x = input_to_hidden(in_values, W_in, b)\n",
        "    y = np.dot(x, W_out)\n",
        "    return y\n",
        "\n",
        "def impute_data(comb_df, well_names, names):\n",
        "    # for out test set we will impute everything\n",
        "    imputed_df = pd.DataFrame(index=comb_df.index)\n",
        "\n",
        "    for well in well_names:  # list of the wells in the aquifer\n",
        "        train_nona_df = comb_df.dropna(\n",
        "            subset=[well]\n",
        "        )  # drop any rows with na in well (measured) data\n",
        "        labels_df = train_nona_df[\n",
        "            well\n",
        "        ]  # measured data used as \"labels\" or truth in training\n",
        "        tx_df = train_nona_df[\n",
        "            names\n",
        "        ]  # data we will predict with only over the test period\n",
        "        all_tx_df = comb_df[names]  # data over the full period, will use for imputation\n",
        "\n",
        "        tx = tx_df.values  # convert to an array\n",
        "        x1 = np.column_stack(np.ones(tx.shape[0])).T  # bias vector of 1's\n",
        "        tx = np.hstack((tx, x1))  # training matrix\n",
        "        ty = labels_df.values\n",
        "        input_length = tx.shape[1]\n",
        "        hidden_units = 500\n",
        "        lamb_value = 100\n",
        "        W_in = np.random.normal(size=[input_length, hidden_units])\n",
        "        b = np.random.normal(size=[hidden_units])\n",
        "\n",
        "        # now do the matrix multiplication\n",
        "        X = input_to_hidden(\n",
        "            tx, W_in, b\n",
        "        )  # setup matrix for multiplication, it is a function\n",
        "        I = np.identity(X.shape[1])\n",
        "        I[X.shape[1] - 1, X.shape[1] - 1] = 0\n",
        "        I[X.shape[1] - 2, X.shape[1] - 2] = 0\n",
        "        W_out = np.linalg.lstsq(X.T.dot(X) + lamb_value * I, X.T.dot(ty), rcond=-1)[0]\n",
        "        all_tx_values = all_tx_df.values\n",
        "        a1 = np.column_stack(np.ones(all_tx_values.shape[0])).T\n",
        "        all_tx_values = np.hstack((all_tx_values, a1))\n",
        "        predict_values = predict(all_tx_values, W_in, b, W_out)  # it is a function\n",
        "        #pre_name = f\"{well}_imputed\"\n",
        "        pre_name = f\"{well}\"\n",
        "        imputed_df[pre_name] = pd.Series(predict_values, index=imputed_df.index)\n",
        "\n",
        "    return imputed_df\n",
        "\n",
        "def renorm_data(in_df, ref_df):\n",
        "    assert in_df.shape[1] == ref_df.shape[1], \"must have same # of columns\"\n",
        "    renorm_df = (\n",
        "        in_df * (ref_df.max().values - ref_df.min().values)\n",
        "    ) + ref_df.min().values\n",
        "    return renorm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAc1hoUEI3wM"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Collect well data that qualifies for use in interpolation**\n",
        "wells_df = pd.concat([\n",
        "        extract_well_data(name, group, min_samples, ts_measurement.value, ts_date.value)\n",
        "        for name, group in measurements.groupby(ts_well_ID.value)],\n",
        "    axis=1, sort=False,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i764sqEJI1F"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Cleaning up wells dataframe**\n",
        "wells_df.drop_duplicates(inplace=True)\n",
        "wells_df[wells_df == 0] = np.nan\n",
        "# wells_df.to_csv(\"wells_one.csv\")\n",
        "wells_df.dropna(thresh=min_samples, axis=1, inplace=True)\n",
        "no_qualifiedwells = len(wells_df.columns.values)\n",
        "no_droppedwells = no_uniquewells - no_qualifiedwells\n",
        "print(no_uniquewellsref,  ' wells with measurements')\n",
        "print(no_droppedwells,  ' wells dropped')\n",
        "print(no_qualifiedwells,  ' qualified for use in interpolation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRiecuiHJObc"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Running simple pchip temporal interpolation**\n",
        "well_interp_df = interp_well(wells_df, gap_size, pad, spacing)\n",
        "well_interp_df.dropna(thresh=min_samples, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOXbYEhnKHM2"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Running machine learning algorithm to fill gaps**\n",
        "# combine the  data from the wells and the satellite observations  to a single dataframe (combined_df)\n",
        "# this will have a row for every measurement (on the start of the month) a column for each well,\n",
        "# and a column for pdsi and soilw and their rolling averages, and potentially offsets\n",
        "combined_df = pd.concat(\n",
        "    [well_interp_df, gldas_df], join=\"outer\", axis=1, sort=False\n",
        ")\n",
        "combined_df.dropna(\n",
        "    subset=names, inplace=True\n",
        ")  # drop rows where there are no satellite data\n",
        "combined_df.dropna(how=\"all\", axis=1, inplace=True)\n",
        "\n",
        "norm_df = norm_training_data(combined_df, combined_df)\n",
        "norm_df.dropna(how=\"all\", axis=1, inplace=True)\n",
        "well_names = [col for col in well_interp_df.columns if col in norm_df.columns]\n",
        "imputed_norm_df = impute_data(norm_df, well_names, names)\n",
        "ref_df = combined_df[well_names]\n",
        "imputed_df = renorm_data(imputed_norm_df, ref_df)\n",
        "\n",
        "imputed_well_names = imputed_df.columns  # create a list of well names\n",
        "loc_well_names = [(strg.replace(\"_imputed\", \"\")) for strg in imputed_well_names]  # strip off \"_imputed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcVDS0wWDmrb"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Merging measurement, pchip, and machine learning dataframes**\n",
        "#formating column names to match so that the dataframes can be combined\n",
        "well_interp_columns = []\n",
        "for i in range(len(well_interp_df.columns)) : well_interp_columns.append(str(well_interp_df.columns[i]))\n",
        "well_interp_df.columns = well_interp_columns\n",
        "\n",
        "imputed_columns = []\n",
        "for i in range(len(imputed_df.columns)) : imputed_columns.append(str(imputed_df.columns[i]))\n",
        "imputed_df.columns = imputed_columns\n",
        "\n",
        "#replacing the machine learning values with pchip values when available\n",
        "replaced_df = well_interp_df.combine_first(imputed_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rei8iiYcSVf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Run Cell and Select the Well you Wish to Plot**\n",
        "pList = imputed_df.columns\n",
        "p_index = widgets.Dropdown(options=pList, description = \"Well Index\")\n",
        "\n",
        "pItems = [p_index]\n",
        "print(\"\\nSelect the well index you wish to plot\")\n",
        "widgets.GridBox(pItems, layout=widgets.Layout(grid_template_columns=\"repeat(1, 550px)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11LdQpfzWhH5"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Plots measurments, pchip, ml timeseries to show complete temporal interpolation**\n",
        "well_no = imputed_df.columns.get_loc(p_index.value)\n",
        "\n",
        "#formatting dates for plotting\n",
        "imputed_date =  pd.to_datetime(imputed_df.index.values)\n",
        "interp_date =  pd.to_datetime(well_interp_df.index.values)\n",
        "wells_date =  pd.to_datetime(wells_df.index.values)\n",
        "replaced_date =  pd.to_datetime(replaced_df.index.values)\n",
        "\n",
        "#ignoring NANs for plotting\n",
        "well_interp_df.dropna()\n",
        "wells_df.dropna()\n",
        "\n",
        "#plotting the three dataframes: measurements, pchip, and machine learning\n",
        "fig, ax = plt.subplots(2, 1, figsize=(25,10))\n",
        "#ax.set_xlim([datetime.date(2001, 1, 26), datetime.date(2021, 2, 1)])\n",
        "ax[0].plot(imputed_date, imputed_df[imputed_df.columns[well_no]]) #machine learning\n",
        "ax[0].plot(interp_date, well_interp_df[well_interp_df.columns[well_no]], color = 'red') #pchip\n",
        "ax[0].plot(wells_date, wells_df[wells_df.columns[well_no]], marker = \"o\", linestyle = 'none', color = 'green') #measurements\n",
        "ax[1].plot(replaced_date, replaced_df[replaced_df.columns[well_no]], color = 'purple')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZlgqfszLNZI"
      },
      "source": [
        "# Spatial Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFIun_lQlTdv"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Setting Parameters for Spatial Interpolation**\n",
        "raster_extent = \"aquifer\" #@param [\"aquifer\", \"wells\"]\n",
        "raster_interval = 12 #@param {type:\"slider\", min:1, max:24, step:1}\n",
        "output_file = \"results.nc\" #@param {type:\"string\"}\n",
        "aquifer_name = \"sunflower\" #@param {type:\"string\"}\n",
        "units = \"acre-ft\" #@param [\"acre-ft\", \"cubic-ft\", \"Cubic Meter\"]\n",
        "start_date = 2001\n",
        "end_date = 2019\n",
        "#let user pick the entire date\n",
        "#then if they pick an inappropriate date, give them a suggestion for the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37y1_38NMNWv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Defining functions for this section**\n",
        "\n",
        "def create_grid_coords(x_c, y_c, x_steps, bbox, raster_extent):\n",
        "    # create grid coordinates fro kriging, make x and y steps the same\n",
        "    # x_steps is the number of cells in the x-direction\n",
        "    min_x = min_y = max_x = max_y = None\n",
        "    if raster_extent == \"aquifer\":\n",
        "        min_x, min_y, max_x, max_y = bbox\n",
        "    elif raster_extent == \"wells\":\n",
        "        min_x, max_x = min(x_c), max(x_c)\n",
        "        min_y, max_y = min(y_c), max(y_c)\n",
        "\n",
        "    n_bin = np.absolute((max_x - min_x) / x_steps)  # determine step size (positive)\n",
        "    # make grid 10 bin steps bigger than date, will give 110 steps in x-direction\n",
        "    grid_x = np.arange(\n",
        "        min_x - 5 * n_bin, max_x + 5 * n_bin, n_bin\n",
        "    )  # make grid 10 steps bigger than data\n",
        "    grid_y = np.arange(\n",
        "        min_y - 5 * n_bin, max_y + 5 * n_bin, n_bin\n",
        "    )  # make grid 10 steps bigger than data\n",
        "\n",
        "    return grid_x, grid_y\n",
        "\n",
        "def krig_field_generate(var_fitted, x_c, y_c, values, grid_x, grid_y):\n",
        "    # use GSTools to krig  the well data, need coords and value for each well\n",
        "    # use model variogram paramters generated by GSTools\n",
        "    # fast - is faster the variogram fitting\n",
        "    krig_map = gs.krige.Ordinary(var_fitted, cond_pos=[x_c, y_c], cond_val=values)\n",
        "    krig_map.structured([grid_x, grid_y], chunk_size=1000)  # krig_map.field is the numpy array of values\n",
        "    return krig_map    \n",
        "\n",
        "def fit_model_var(x_c, y_c, values, bbox, raster_extent):\n",
        "    # fit the model varigrom to the experimental variogram\n",
        "    min_x = min_y = max_x = max_y = None\n",
        "    if raster_extent == \"aquifer\":\n",
        "        min_x, min_y, max_x, max_y = bbox\n",
        "    elif raster_extent == \"wells\":\n",
        "        min_x, max_x = min(x_c), max(x_c)\n",
        "        min_y, max_y = min(y_c), max(y_c)\n",
        "\n",
        "    # first get the coords and determine distances\n",
        "    x_delta = max_x - min_x  # distance across x coords\n",
        "    y_delta = max_y - min_y  # distance across y coords\n",
        "    max_dist = (\n",
        "        np.sqrt(np.square(x_delta + y_delta)) / 4\n",
        "    )  # assume correlated over 1/4 of distance\n",
        "    data_var = np.var(values)\n",
        "    data_std = np.std(values)\n",
        "    fit_var = gs.Stable(dim=2, var=data_var, len_scale=max_dist, nugget=data_std)\n",
        "    return fit_var\n",
        "\n",
        "def generate_nc_file(\n",
        "    file_name, grid_x, grid_y, years_df, x_coords, y_coords, bbox, raster_extent\n",
        "):\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    file_path = os.path.join(temp_dir, file_name)\n",
        "    h = netCDF4.Dataset(file_path, \"w\", format=\"NETCDF4\")\n",
        "    lat_len = len(grid_y)\n",
        "    lon_len = len(grid_x)\n",
        "    time_dim = h.createDimension(\"time\", 0)\n",
        "    lat = h.createDimension(\"lat\", lat_len)\n",
        "    lon = h.createDimension(\"lon\", lon_len)\n",
        "    latitude = h.createVariable(\"lat\", np.float64, (\"lat\"))\n",
        "    longitude = h.createVariable(\"lon\", np.float64, (\"lon\"))\n",
        "    time_dim = h.createVariable(\"time\", np.float64, (\"time\"), fill_value=\"NaN\")\n",
        "    ts_value = h.createVariable(\n",
        "        \"tsvalue\", np.float64, (\"time\", \"lon\", \"lat\"), fill_value=-9999\n",
        "    )\n",
        "    latitude.long_name = \"Latitude\"\n",
        "    latitude.units = \"degrees_north\"\n",
        "    latitude.axis = \"Y\"\n",
        "    longitude.long_name = \"Longitude\"\n",
        "    longitude.units = \"degrees_east\"\n",
        "    longitude.axis = \"X\"\n",
        "    time_dim.axis = \"T\"\n",
        "    time_dim.units = \"days since 0001-01-01 00:00:00 UTC\"\n",
        "\n",
        "    latitude[:] = grid_y[:]\n",
        "    longitude[:] = grid_x[:]\n",
        "\n",
        "    time_counter = 0\n",
        "    for measurement in years_df:\n",
        "        # loop through the data\n",
        "        values = years_df[measurement].values\n",
        "\n",
        "        beg_time = timer()  # time the kriging method including variogram fitting\n",
        "        # fit the model variogram to the experimental variogram\n",
        "        var_fitted = fit_model_var(\n",
        "            x_coords, y_coords, values, bbox, raster_extent\n",
        "        )  # fit variogram\n",
        "        krig_map = krig_field_generate(\n",
        "            var_fitted, x_coords, y_coords, values, grid_x, grid_y\n",
        "        )  # krig data\n",
        "        # krig_map.field provides the 2D array of values\n",
        "        end_time = timer()\n",
        "        time_dim[time_counter] = measurement.toordinal()\n",
        "        ts_value[time_counter, :, :] = krig_map.field\n",
        "        time_counter += 1\n",
        "\n",
        "    h.close()\n",
        "    return Path(file_path)\n",
        "\n",
        "def earth_radius(lat):\n",
        "    \"\"\"\n",
        "    calculate radius of Earth assuming oblate spheroid\n",
        "    defined by WGS84\n",
        "    Input\n",
        "    ---------\n",
        "    lat: vector or latitudes in degrees\n",
        "    Output\n",
        "    ----------\n",
        "    r: vector of radius in meters\n",
        "    Notes\n",
        "    -----------\n",
        "    WGS84: https://earth-info.nga.mil/GandG/publications/tr8350.2/tr8350.2-a/Chapter%203.pdf\n",
        "    Taken from: https://gist.github.com/lgloege/3fdb1ed83b002d68d8944539a797b0bc\n",
        "    \"\"\"\n",
        "    from numpy import deg2rad\n",
        "\n",
        "    # define oblate spheroid from WGS84\n",
        "    a = 6378137\n",
        "    b = 6356752.3142\n",
        "    e2 = 1 - (b ** 2 / a ** 2)\n",
        "\n",
        "    # convert from geodecic to geocentric\n",
        "    # see equation 3-110 in WGS84\n",
        "    lat = deg2rad(lat)\n",
        "    lat_gc = np.arctan((1 - e2) * np.tan(lat))\n",
        "\n",
        "    # radius equation\n",
        "    # see equation 3-107 in WGS84\n",
        "    r = (a * (1 - e2) ** 0.5) / (1 - (e2 * np.cos(lat_gc) ** 2)) ** 0.5\n",
        "\n",
        "    return r\n",
        "\n",
        "def calculate_aquifer_area(imputed_raster, units):\n",
        "    y_res = abs(\n",
        "        round(imputed_raster[\"lat\"].values[0] - imputed_raster[\"lat\"].values[1], 7)\n",
        "    )  # this assumes all cells will be the same\n",
        "    # size in one dimension (all cells will have same x-component)\n",
        "    x_res = abs(\n",
        "        round(imputed_raster[\"lon\"].values[0] - imputed_raster[\"lon\"].values[1], 7)\n",
        "    )\n",
        "    area = 0\n",
        "    # Loop through each y row\n",
        "    for y in range(imputed_raster.lat.size):\n",
        "        # Define the upper and lower bounds of the row\n",
        "        cur_lat_max = math.radians(imputed_raster[\"lat\"].values[y] + (y_res / 2))\n",
        "        cur_lat_min = math.radians(imputed_raster[\"lat\"].values[y] - (y_res / 2))\n",
        "\n",
        "        # Count how many cells in each row are in aquifer (i.e. and, therefore, not nan)\n",
        "        x_count = np.count_nonzero(~np.isnan(imputed_raster.tsvalue[0, :, y]))\n",
        "\n",
        "        # Area calculated based on the equation found here:\n",
        "        # https://www.pmel.noaa.gov/maillists/tmap/ferret_users/fu_2004/msg00023.html\n",
        "        #     (pi/180) * R^2 * |lon1-lon2| * |sin(lat1)-sin(lat2)|\n",
        "        radius = earth_radius(imputed_raster[\"lat\"].values[y])\n",
        "        if units == \"acre-ft\":\n",
        "            area_factor = 1 / 4046.8564224\n",
        "        elif units == \"cubic-ft\":\n",
        "            area_factor = 10.764\n",
        "        else:\n",
        "            area_factor = 1\n",
        "        area += (\n",
        "            radius ** 2\n",
        "            * area_factor\n",
        "            * math.radians(x_res * x_count)\n",
        "            * abs((math.sin(cur_lat_min) - math.sin(cur_lat_max)))\n",
        "        )\n",
        "\n",
        "    return area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcBFa86OLMF2"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Setting up grid for nc file**\n",
        "coords_df = wells_gdf[wells_gdf[w_well_ID.value].astype(str).isin(loc_well_names)]\n",
        "x_coords = coords_df[w_long.value].values\n",
        "y_coords = coords_df[w_lat.value].values\n",
        "\n",
        "x_steps = 150  # steps in x-direction, number of y-steps will be computed with same spacing, adds 10%\n",
        "grid_x, grid_y = create_grid_coords(\n",
        "    x_coords, y_coords, x_steps, bbox, raster_extent\n",
        ")  # coordinates for x and y axis - not full grid\n",
        "replaced_df = replaced_df[\n",
        "    (replaced_df.index >= f\"04-01-{start_date-1}\")\n",
        "    & (replaced_df.index <= f\"12-31-{end_date+1}\")\n",
        "]\n",
        "# skip_month = 48  # take data every nth month (skip_months), e.g., 60 = every 5 years\n",
        "years_df = replaced_df.iloc[\n",
        "    ::raster_interval\n",
        "].T  # extract every nth month of data and transpose array\n",
        "\n",
        "file_name = f\"{aquifer_name}_{time.time()}.nc\"\n",
        "# setup a netcdf file to store the time series of rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auxwnaz0MhaP"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Generate nc file using kriging**\n",
        "nc_file_path = generate_nc_file(file_name, grid_x, grid_y, years_df, x_coords, y_coords, bbox, raster_extent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e26WwYjhNNFe"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Assigning dimensions and clipping nc file**\n",
        "file_path = nc_file_path\n",
        "temp_dir = file_path.parent.absolute()\n",
        "interp_nc = xarray.open_dataset(file_path)\n",
        "interp_nc.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
        "interp_nc.rio.write_crs(\"epsg:4326\", inplace=True)\n",
        "clipped_nc = interp_nc.rio.clip(aquifer.geometry.apply(mapping), crs=4326, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqTj_U06NlRE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Calculating drawdown**\n",
        "area = calculate_aquifer_area(clipped_nc, units)\n",
        "if units == \"acre-ft\":\n",
        "    vol_unit = \"Acre Feet\"\n",
        "elif units == \"cubic-ft\":\n",
        "    vol_unit = \"Cubic Feet\"\n",
        "else:\n",
        "    vol_unit = \"Cubic Meters\"\n",
        "# Calculate total drawdown volume at each time step\n",
        "drawdown_grid = np.zeros(\n",
        "    (clipped_nc.time.size, clipped_nc.lon.size, clipped_nc.lat.size)\n",
        ")\n",
        "drawdown_volume = np.zeros(clipped_nc.time.size)\n",
        "for t in range(clipped_nc.time.size):\n",
        "    # Calculate drawdown at time t by subtracting original WTE at time 0\n",
        "    drawdown_grid[t, :, :] = (\n",
        "        clipped_nc[\"tsvalue\"][t, :, :] - clipped_nc[\"tsvalue\"][0, :, :]\n",
        "    )\n",
        "    # Average drawdown across entire aquifer x storage_coefficient x area of aquifer\n",
        "    drawdown_volume[t] = np.nanmean(\n",
        "        drawdown_grid[t, :, :] * 0.1 * area\n",
        "    )\n",
        "print(area, units)\n",
        "print(type(clipped_nc))\n",
        "clipped_nc.attrs = {\"area\": area, \"area_units\": units}\n",
        "clipped_nc[\"drawdown\"] = ([\"time\", \"lon\", \"lat\"], drawdown_grid)\n",
        "clipped_nc[\"volume\"] = ([\"time\"], drawdown_volume, {\"units\": vol_unit})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw8LQkQUOCsX"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Saving to notebook**\n",
        "clipped_nc.to_netcdf(aquifer_name + \"_\"+ output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJQqMoirIt8-"
      },
      "source": [
        "# Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSKToMJ5HDJ3"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Plotting the Drawdown Curve**\n",
        "nc = xarray.open_dataset('/content/'+aquifer_name + \"_\"+ output_file)\n",
        "time = nc.variables['time'][:]\n",
        "volume = nc.variables['volume'][:]\n",
        "ts = nc.variables['tsvalue'][:]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(25,5))\n",
        "ax.plot(time, volume)\n",
        "plt.xlabel(\"Date (years)\")\n",
        "plt.ylabel(\"Volume (\"+ vol_unit+\")\")\n",
        "plt.title(aquifer_name.capitalize() + \" Drawdown Curve\")\n",
        "### don't clip it so we can see the whole curve from machine learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Export drawdown curve to CSV\n",
        "drawdown_df = pd.DataFrame({'Time': time, 'Volume': volume})\n",
        "drawdown_df.to_csv(aquifer_name + \"_\" + \"drawdown.csv\")"
      ],
      "metadata": {
        "id": "X9UWqEfIINb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install basemap\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "import numpy\n",
        "from os import path\n",
        "import glob\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "I2m7vkhpXuMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leftbound = bbox[0]\n",
        "bottombound = bbox[1]\n",
        "rightbound = bbox[2]\n",
        "topbound = bbox[3]\n",
        "Frame_Cushion = 0.25"
      ],
      "metadata": {
        "id": "J0_gvTEGeMgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up the time index for your GIF\n",
        "starttime = numpy.datetime64(f\"{start_date+1}-04-01\", 'M')\n",
        "endtime = numpy.datetime64(f\"{end_date-1}-12-31\", 'M')\n",
        "startindex, endindex = -1, -1\n",
        "for i in range(len(nc['time'][:])-1):\n",
        "  check = numpy.datetime64(nc.coords['time'].values[i], 'M')\n",
        "  if starttime < check and startindex == -1:\n",
        "    startindex = i-1\n",
        "  if endtime < check and endindex == -1: \n",
        "    endindex = i+1"
      ],
      "metadata": {
        "id": "ZCDKuRweO5ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "if path.exists('/content/images_gif') == False:\n",
        "  os.mkdir('/content/images_gif')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "aquifer.plot(color=\"none\", edgecolor=\"red\", ax=ax)\n",
        "m = Basemap(projection='cyl', llcrnrlon=leftbound-Frame_Cushion, llcrnrlat=bottombound-Frame_Cushion, urcrnrlon=rightbound+Frame_Cushion, urcrnrlat=topbound+Frame_Cushion, resolution='i', epsg=4326)\n",
        "EsriBasemaps = \"World_Topo_Map\" #@param [\"NatGeo_World_Map\", \"USA_Topo_Maps\", \"World_Imagery\", \"World_Physical_Map\", \"World_Shaded_Relief\", \"World_Street_Map\", \"World_Terrain_Base\", \"World_Topo_Map\"]\n",
        "m.arcgisimage(server='http://server.arcgisonline.com/ArcGIS', service=EsriBasemaps, xpixels = 400, dpi = 69,verbose = True)\n",
        "\n",
        "for i in range(startindex,endindex):\n",
        "  im = plt.imshow(numpy.rot90(ts[i,:,:]), cmap=\"Spectral\",extent=(leftbound, rightbound, bottombound, topbound) ,vmin=numpy.amin(ts),vmax=numpy.amax(ts))\n",
        "  plt.title(aquifer_name.capitalize()+\" \"+str(nc.coords['time'].values[i])[:10], pad = 15)\n",
        "  if i==startindex:  \n",
        "    cbar = plt.colorbar()\n",
        "    cbar.set_label(ts_measurement.value+\" in \"+units, labelpad = 20)\n",
        "  plt.savefig(f\"/content/images_gif/image'{str(nc.coords['time'].values[i])[:10]}.jpg\")"
      ],
      "metadata": {
        "cellView": "code",
        "id": "9HFQb2dbVS6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_duration=300\n",
        "frames = []\n",
        "imgs=glob.glob(f\"/content/images_gif/*.jpg\")\n",
        "imgs.sort()\n",
        "for i in imgs:\n",
        "    new_frame = Image.open(i)\n",
        "    frames.append(new_frame)\n",
        "    frames[0].save(aquifer_name + '_animation.gif', format='GIF', append_images=frames[1:],save_all=True, duration = frame_duration, loop=0)"
      ],
      "metadata": {
        "id": "GkNIO8kOV4De"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}